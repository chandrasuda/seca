# ── SECA: Selective Externalization for Continual Adaptation ──

model:
  name: "meta-llama/Llama-3.2-1B-Instruct"   # base LM
  dtype: "bfloat16"
  max_len: 1024

sdft:
  num_on_policy_samples: 4        # K rollouts per demo
  temperature_student: 1.0
  temperature_teacher: 0.7
  kl_weight: 0.1                  # β for reverse-KL term
  lr: 1e-5
  epochs_per_episode: 1
  max_grad_norm: 1.0

router:
  hidden_dim: 256
  feature_dim: null               # auto from model hidden size
  lr: 3e-4
  baseline_ema: 0.99              # REINFORCE variance reduction

reward:
  lambda_forget: 1.0              # forgetting penalty weight
  mu_cost: 0.01                   # update cost penalty

memory:
  embedding_model: "BAAI/bge-small-en-v1.5"
  top_k: 5
  chunk_size: 256
  chunk_overlap: 32

data:
  skills_path: "data/skills/"
  knowledge_path: "data/knowledge/"
  anchor_path: "data/anchor/"
  stream_length: 100
  skill_ratio: 0.5                # fraction of skill episodes

eval:
  anchor_eval_every: 5            # evaluate anchor suite every N episodes
  log_dir: "logs/"

seed: 42
